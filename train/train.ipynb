{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Config","metadata":{"id":"LBH3tsiIPDjB"}},{"cell_type":"code","source":"class CONFIG:\n    batch_size = 128 # PARAM\n    val_ratio = 0.1 # PARAM\n    base_layer_name = 'resnet34' # PARAM\n    final_layer_n_classes = 37\n    num_epochs = 25 # PARAM\n    optimizer_lr = 0.001\n    scheduler_step_size = 7\n    scheduler_gamma = 0.1\n    def model_log_name(self):\n        return f\"LoRA_Pet_{CONFIG.base_layer_name}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T12:03:26.072741Z","iopub.execute_input":"2024-12-09T12:03:26.073692Z","iopub.status.idle":"2024-12-09T12:03:26.078809Z","shell.execute_reply.started":"2024-12-09T12:03:26.073643Z","shell.execute_reply":"2024-12-09T12:03:26.077694Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#!pip install torchvision torch\n!pip install comet_ml","metadata":{"id":"efc801a8","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:37:49.147358Z","iopub.execute_input":"2024-12-09T09:37:49.147830Z","iopub.status.idle":"2024-12-09T09:38:03.864737Z","shell.execute_reply.started":"2024-12-09T09:37:49.147789Z","shell.execute_reply":"2024-12-09T09:38:03.862833Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import OxfordIIITPet\nfrom torchvision import models, transforms\nfrom torch.utils.data import DataLoader\nimport os\nfrom torch.optim import Adam","metadata":{"id":"eCa3rXOjJnA9","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:38:03.867226Z","iopub.execute_input":"2024-12-09T09:38:03.867627Z","iopub.status.idle":"2024-12-09T09:38:08.477820Z","shell.execute_reply.started":"2024-12-09T09:38:03.867587Z","shell.execute_reply":"2024-12-09T09:38:08.476597Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data","metadata":{}},{"cell_type":"code","source":"transform = transforms.Compose([\n    transforms.Resize((224, 224)),  # Resize images to fit the model input size\n    transforms.ToTensor(),\n])\n\n# Load the dataset\ntrain_dataset = OxfordIIITPet(root='data/', split='trainval', download=True, transform=transform)\ntest_dataset = OxfordIIITPet(root='data/', split='test', download=True, transform=transform)","metadata":{"id":"_ra5IOvYJuaL","outputId":"8d935377-f33a-415f-e5f7-e09e3c0a5653","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:38:08.479031Z","iopub.execute_input":"2024-12-09T09:38:08.479507Z","iopub.status.idle":"2024-12-09T09:38:24.518665Z","shell.execute_reply.started":"2024-12-09T09:38:08.479469Z","shell.execute_reply":"2024-12-09T09:38:24.517322Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom torchvision.transforms import ToPILImage\n\n# Assuming 'train_dataset' and 'test_dataset' are already loaded\n\n# Print the size of train and test datasets\nprint(f\"Size of Training Dataset: {len(train_dataset)}\")\nprint(f\"Size of Test Dataset: {len(test_dataset)}\")\n\n# Display total labels (assuming label information is available in the dataset)\ntotal_labels = len(np.unique([label for _, label in train_dataset]))\nprint(f\"Total Labels: {total_labels}\")\n\n# Function to show sample images for 5 labels\ndef show_sample_images(dataset, num_labels=5):\n    fig, axs = plt.subplots(1, num_labels, figsize=(15, 3))\n    label_samples = {}\n\n    for image, label in dataset:\n        # Convert tensor image to PIL for display\n        image = ToPILImage()(image)\n\n        if label not in label_samples:\n            label_samples[label] = image\n        if len(label_samples) == num_labels:\n            break\n\n    for i, (label, image) in enumerate(label_samples.items()):\n        axs[i].imshow(image)\n        axs[i].set_title(f\"Label: {label}\")\n        axs[i].axis('off')\n\n    plt.show()\n\n# Show sample images from the training dataset\nshow_sample_images(train_dataset)\n","metadata":{"id":"d6cc341a","outputId":"f031be64-7ba9-4bb0-e553-b2de5cce7b85","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:38:24.521953Z","iopub.execute_input":"2024-12-09T09:38:24.522485Z","iopub.status.idle":"2024-12-09T09:38:48.641215Z","shell.execute_reply.started":"2024-12-09T09:38:24.522431Z","shell.execute_reply":"2024-12-09T09:38:48.639837Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DataLoader","metadata":{"id":"ba0a3f6c"}},{"cell_type":"markdown","source":"Batch Size Recommendations:\n\n1.\tResNet-18 or ResNet-34:\n\n* Start with a batch size of 128.\n* Increase to 256 if VRAM permits or use mixed precision training for optimization.\n\n2.\tResNet-50 or ResNet-101:\n\n* Start with a batch size of 64.\n* Use gradient accumulation to simulate larger batch sizes if needed.\n","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom torch.utils.data import random_split\n\n\nbatch_size = CONFIG.batch_size # PARAM\nval_ratio = CONFIG.val_ratio # PARAM\n\n\n# Define the size of the validation set\nval_size = int(val_ratio * len(train_dataset))\ntrain_size = len(train_dataset) - val_size\n\n# Split the dataset\ntrain_data, val_data = random_split(train_dataset, [train_size, val_size])\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"DdrghwHwLWMK","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:38:48.642661Z","iopub.execute_input":"2024-12-09T09:38:48.643048Z","iopub.status.idle":"2024-12-09T09:38:48.663843Z","shell.execute_reply.started":"2024-12-09T09:38:48.643009Z","shell.execute_reply":"2024-12-09T09:38:48.662409Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"markdown","source":"## COMET","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:38:48.665541Z","iopub.execute_input":"2024-12-09T09:38:48.666028Z","iopub.status.idle":"2024-12-09T09:38:48.671623Z","shell.execute_reply.started":"2024-12-09T09:38:48.665977Z","shell.execute_reply":"2024-12-09T09:38:48.670351Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import comet_ml\nexperiment = comet_ml.start(\n  api_key=user_secrets.get_secret(\"comet_api_key\"),\n  project_name=\"pet-recognition\",\n  workspace=user_secrets.get_secret(\"comet_workspace\")\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:38:48.673832Z","iopub.execute_input":"2024-12-09T09:38:48.674325Z","iopub.status.idle":"2024-12-09T09:38:55.681324Z","shell.execute_reply.started":"2024-12-09T09:38:48.674270Z","shell.execute_reply":"2024-12-09T09:38:55.679291Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\nfrom torchvision.models import get_model, get_model_weights\n\n# Load a pre-trained ResNet model\nbase_layer_name = CONFIG.base_layer_name # PARAM\nmodel = get_model(name=base_layer_name, weights=\"DEFAULT\")\nexperiment.log_parameter('base_layer_name', base_layer_name)\n\nnum_ftrs = model.fc.in_features\n\n# Adjust the final layer for 37 classes\nfinal_layer_n_classes = CONFIG.final_layer_n_classes\nexperiment.log_parameter('final_layer_n_classes', final_layer_n_classes)\nmodel.fc = nn.Linear(num_ftrs, final_layer_n_classes)\n\n# LORA adaptation\nclass LORALayer(nn.Module):\n    def __init__(self, adapted_layer, rank=16):\n        super(LORALayer, self).__init__()\n        self.adapted_layer = adapted_layer\n        self.A = nn.Parameter(torch.randn(adapted_layer.weight.size(1), rank))\n        self.B = nn.Parameter(torch.randn(rank, adapted_layer.weight.size(0)))\n\n    def forward(self, x):\n        low_rank_matrix = self.A @ self.B\n        adapted_weight = self.adapted_layer.weight + low_rank_matrix.t()  # Ensure correct shape\n        return nn.functional.linear(x, adapted_weight, self.adapted_layer.bias)\n\n# Apply LORA to the last layer of the model\nmodel.fc = LORALayer(model.fc)","metadata":{"id":"d8e143dc","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:38:55.683781Z","iopub.execute_input":"2024-12-09T09:38:55.684168Z","iopub.status.idle":"2024-12-09T09:38:56.403721Z","shell.execute_reply.started":"2024-12-09T09:38:55.684123Z","shell.execute_reply":"2024-12-09T09:38:56.402571Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train","metadata":{}},{"cell_type":"code","source":"from torch.optim import lr_scheduler\n\n# PARAM\nexperiment.log_parameter('num_epochs', CONFIG.num_epochs)\n\n\n# Check if CUDA (GPU support) is available and use it; otherwise, fall back to CPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nexperiment.log_parameter('device_train', device)\n\n# Move the model to the selected device\nmodel = model.to(device)\n\n# Track losses and accuracies\ntrain_losses = []\nval_losses = []\ntest_losses = []\n\ntrain_accuracies = []\nval_accuracies = []\ntest_accuracies = []\n\noptimizer = torch.optim.Adam(model.parameters(), lr=CONFIG.optimizer_lr)\nexperiment.log_parameter('optimizer_lr', CONFIG.optimizer_lr)\nexperiment.log_parameter('optimizer', \"Adam\")\n\ncriterion = nn.CrossEntropyLoss()\nexperiment.log_parameter('criterion', \"CrossEntropyLoss\")\n\n\n# Implement a learning rate scheduler\nscheduler = lr_scheduler.StepLR(optimizer, step_size=CONFIG.scheduler_step_size, gamma=CONFIG.scheduler_gamma) # PARAM\nexperiment.log_parameter('scheduler_step_size', CONFIG.scheduler_step_size)\nexperiment.log_parameter('scheduler_gamma', CONFIG.scheduler_gamma)\n\nbest_val_accuracy = 0.0\nbest_model_wts = model.state_dict()\n\nfor epoch in range(CONFIG.num_epochs):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n\n    for images, labels in train_loader:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n    scheduler.step()  # Adjust the learning rate based on the scheduler\n    train_accuracy = 100 * correct / total\n    train_losses.append(running_loss / len(train_loader))\n    train_accuracies.append(train_accuracy)\n\n    # Validation phase\n    model.eval()\n    val_running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            val_running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    val_accuracy = 100 * correct / total\n    val_losses.append(val_running_loss / len(val_loader))\n    val_accuracies.append(val_accuracy)\n\n    # Test phase\n    model.eval()\n    test_running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            test_running_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    test_accuracy = 100 * correct / total\n    test_losses.append(test_running_loss / len(test_loader))\n    test_accuracies.append(test_accuracy)\n\n    # Save the model if validation accuracy improves\n    if val_accuracy > best_val_accuracy:\n        best_val_accuracy = val_accuracy\n        best_model_wts = model.state_dict()\n\n    train_loss_epoch = running_loss / len(train_loader)\n    val_loss_epoch = val_running_loss / len(val_loader)\n    test_running_loss = test_running_loss / len(test_loader)\n                                            \n    experiment.log_metrics({\n        \"Train Loss Epoch\": train_loss_epoch,\n        \"Train Accuracy Epoch\": train_accuracy,\n        \"Val Loss Epoch\": val_loss_epoch,\n        \"Val Accuracy Epoch\": val_accuracy,\n        \"Test Loss Epoch\": val_loss_epoch,\n        \"Test Accuracy Epoch\": test_accuracy\n    }, epoch = epoch)\n\n    print(f'Epoch {epoch}, Train Loss: {train_loss_epoch}, Train Accuracy: {train_accuracy}%, Val Loss: {val_loss_epoch}, Val Accuracy: {val_accuracy}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T09:38:56.404979Z","iopub.execute_input":"2024-12-09T09:38:56.405792Z","iopub.status.idle":"2024-12-09T09:55:30.590746Z","shell.execute_reply.started":"2024-12-09T09:38:56.405753Z","shell.execute_reply":"2024-12-09T09:55:30.589400Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Save","metadata":{}},{"cell_type":"code","source":"from comet_ml.integration.pytorch import log_model\n\n# Load the best model weights\nmodel.load_state_dict(best_model_wts)\n\n# Log the pytorch model to Comet\nlog_model(\n    experiment=experiment,\n    model=model,\n    model_name=CONFIG().model_log_name()\n)\n\nexperiment.register_model(CONFIG().model_log_name())","metadata":{"id":"iw-duEPrGq_p","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:08:55.600515Z","iopub.execute_input":"2024-12-09T10:08:55.601440Z","iopub.status.idle":"2024-12-09T10:08:55.938409Z","shell.execute_reply.started":"2024-12-09T10:08:55.601375Z","shell.execute_reply":"2024-12-09T10:08:55.937166Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Test","metadata":{}},{"cell_type":"code","source":"# Test phase (after training is complete and best model is loaded)\nmodel.eval()\ncorrect = 0\ntotal = 0\nfig = plt.figure(figsize=(25, 5))  # Define figure size\n\n# We will visualize the first 10 images of the test set\nfor i, (images, labels) in enumerate(test_loader, start=1):\n    if i > 10:  # Stop after visualizing 10 images\n        break\n    images, labels = images.to(device), labels.to(device)\n    outputs = model(images)\n    _, predicted = torch.max(outputs, 1)\n\n    total += labels.size(0)\n    correct += (predicted == labels).sum().item()\n\n    ax = fig.add_subplot(2, 5, i)  # Plotting 10 images in 2 rows and 5 columns\n    ax.imshow(images[0].cpu().numpy().transpose((1, 2, 0)))\n    ax.set_title(f\"True: {labels[0].item()}, Pred: {predicted[0].item()}\")\n    ax.axis('off')\n\ntest_accuracy = 100 * correct / total\nprint(f'Test Accuracy: {test_accuracy}%')\nplt.show()\n","metadata":{"id":"698c7922","outputId":"065db04f-ca4c-426a-ebf0-00552e64e88e","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:13:45.845436Z","iopub.execute_input":"2024-12-09T10:13:45.845871Z","iopub.status.idle":"2024-12-09T10:15:01.438454Z","shell.execute_reply.started":"2024-12-09T10:13:45.845831Z","shell.execute_reply":"2024-12-09T10:15:01.436850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tqdm import tqdm\n# import tensorflow as tf\n\nlabels_all = None\npredicted_all = None\nfor i, (images, labels) in tqdm(enumerate(test_loader, start=1)):\n    images, labels = images.to(device), labels.to(device)\n    \n    if labels_all is None:\n        labels_all = labels\n    else:\n        labels_all = torch.cat((labels_all, labels))\n\n    outputs = model(images)\n    _, predicted = torch.max(outputs, 1)\n\n    if predicted_all is None:\n        predicted_all = predicted\n    else:\n        predicted_all = torch.cat((predicted_all, predicted))\n    \n    \ntotal = labels_all.size(0)\ncorrect = (predicted_all == labels_all).sum().item()\ntest_accuracy = 100 * correct / total\n\nexperiment.log_metric(\"Test Accuracy\", test_accuracy, step=None)\nexperiment.log_confusion_matrix(labels_all, predicted_all)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:36:48.043840Z","iopub.execute_input":"2024-12-09T10:36:48.044301Z","iopub.status.idle":"2024-12-09T10:37:25.104377Z","shell.execute_reply.started":"2024-12-09T10:36:48.044262Z","shell.execute_reply":"2024-12-09T10:37:25.103057Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# End Experiment","metadata":{}},{"cell_type":"code","source":"experiment.end()","metadata":{"id":"85U2wAFAGvfe","trusted":true,"execution":{"iopub.status.busy":"2024-12-09T10:50:54.266850Z","iopub.execute_input":"2024-12-09T10:50:54.267339Z","iopub.status.idle":"2024-12-09T10:50:56.341464Z","shell.execute_reply.started":"2024-12-09T10:50:54.267300Z","shell.execute_reply":"2024-12-09T10:50:56.340281Z"}},"outputs":[],"execution_count":null}]}